{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import re\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, trigrams\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open('corpus.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        corpus.append(line[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer and wordlist in case one wants to filter on specific words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_list = []\n",
    "with open('word_file.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        word_list.append(line[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam_filter = keras.models.load_model('spam_filter')\n",
    "#word2idx = np.load('word2idx.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a treebank PoS tag to a Wordnet PoS tag \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent_tokens):\n",
    "    sent_idxs = [word2idx.get(w,1) for w in sent_tokens]\n",
    "    sent_padded = pad_sequences([sent_idxs], maxlen=15)\n",
    "    pred = spam_filter.predict([sent_padded])\n",
    "    return np.argmax(pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b6eed91a344d36ba3226a35fe668f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1931.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick out sentences from the article corpus by checking all conditions\n",
    "sent_corpus = []\n",
    "\n",
    "for article in tqdm(set(corpus[:2000]), position=0, leave=True):\n",
    "    for sentence in sent_tokenize(article):\n",
    "        \n",
    "        stripped_sentence = sentence.strip('\"')\n",
    "        \n",
    "        sent_tokens = word_tokenize(stripped_sentence)\n",
    "        \n",
    "        # Complete sentence corpus\n",
    "        if (4 < len(sent_tokens) <= 15 and \n",
    "            '??' not in sentence and \n",
    "            '\\xa0' not in sentence and\n",
    "            stripped_sentence[-1] in string.punctuation and\n",
    "            not re.findall('[A-Z]{5,}', sentence) and\n",
    "            not stripped_sentence[0].islower()):\n",
    "        \n",
    "            # Standard sent corpus w/o additional filtering\n",
    "            sent_corpus.append([stripped_sentence])    \n",
    "            \n",
    "        \n",
    "            # Further filtered sentence corpus using word lemmas and PoS tags:\n",
    "        \n",
    "            #pos_tags = pos_tag(sent_tokens)\n",
    "            \n",
    "            #approved = [lemmatizer.lemmatize(sent_tokens[i], pos=get_wordnet_pos(pos_tags[i][1])) in word_list \n",
    "             #           or sent_tokens[i] in string.punctuation \n",
    "              #          or pos_tags[i][1] == 'NNP'\n",
    "               #         for i in range(len(sent_tokens))]\n",
    "            \n",
    "            #if all(approved):\n",
    "             #   sent_corpus.append([stripped_sentence])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but using the spam filter model\n",
    "#sent_corpus = []\n",
    "\n",
    "#for article in tqdm(set(corpus[:100]), position=0, leave=True):\n",
    " #   for sentence in sent_tokenize(article):\n",
    "  #      sent_tokens = word_tokenize(sentence)\n",
    "        \n",
    "   #     if 4 < len(sent_tokens) <= 15:\n",
    "    #        if predict(sent_tokens) == 1:\n",
    "     #           sent_corpus.append(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7328"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5acc94360bc49148b1b4ca0d63d9727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=364227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV file\n",
    "#with open('complete_sent_corpus.csv', 'w', newline='\\n') as csvfile:\n",
    " #   wr = csv.writer(csvfile)\n",
    "  #  for line in tqdm(sent_corpus):\n",
    "   #     wr.writerow(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trigrams for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract n-grams from corpus to judge possibility of sentences\n",
    "word_trigrams = {}\n",
    "\n",
    "for article in tqdm(set(corpus), position=0, leave=True):\n",
    "    for sentence in sent_tokenize(article):\n",
    "        sent_tokens = word_tokenize(sentence)\n",
    "        trigr_list = trigrams(sent_tokens)\n",
    "        for trigr in trigr_list:\n",
    "            if trigr in word_trigrams.keys():\n",
    "                word_trigrams[trigr] += 1\n",
    "            else:\n",
    "                word_trigrams[trigr] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17503375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_trigrams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_word_trigrams = {}\n",
    "for key, val in tqdm(word_trigrams.items()):\n",
    "    if val > 1:\n",
    "        reduced_word_trigrams.update({key: val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4826155"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_word_trigrams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trigrams to use later when picking sentences with BERT\n",
    "# np.save('trigrams.npy', reduced_word_trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
